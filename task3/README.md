# Потоковая аналитика

Раз конкретная тема не задана, то producer будет кидать в kafka json'ы вида:
```json
{"event_time": "2025-06-17T19:29:00Z", "device_id": "dev-42","metric": "temp", "value": 23.4}
```

— что-то вроде событий от модулей системы умного дома.

### Как выполнялось

1. Создаются кластеры Managed Service for Kafka и Yandex Data Processing
2. Пишется скрипт продюсера (`producer.py`), который кладет в Kafka событие
3. Пишется PySpark задание - это `consumer.py`, который читает из Kafka, агрегирет данные
и пишет результат в csv файл в Object Storage.
4. Задание запускается аналогично предыдущей задаче